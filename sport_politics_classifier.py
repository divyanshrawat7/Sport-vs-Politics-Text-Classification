# -*- coding: utf-8 -*-
"""sport_politics_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TdAFM_vQU0WeEDOh5n9ob6fBlEn77ima
"""

# importing required modules from sklearn
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns



# choosing only those categories which belong to sport and politics
chosen_topics = [
    'rec.sport.baseball',
    'rec.sport.hockey',
    'talk.politics.guns',
    'talk.politics.mideast',
    'talk.politics.misc'
]


# loading the dataset and removing extra metadata
# headers/footers removed so model does not learn from author names etc.
loaded_pack = fetch_20newsgroups(
    subset='all',
    categories=chosen_topics,
    remove=('headers', 'footers', 'quotes')
)

all_text_data = loaded_pack.data
target_index_values = loaded_pack.target
target_name_list = loaded_pack.target_names


# converting multi-category labels into only two classes
# here I manually map categories into SPORT and POLITICS
final_class_labels = []

for each_id in target_index_values:
    original_label_name = target_name_list[each_id]

    # checking if label contains word sport
    if "sport" in original_label_name:
        final_class_labels.append("SPORT")
    else:
        final_class_labels.append("POLITICS")


# dividing dataset into training and testing portions
train_text_block, test_text_block, train_label_block, test_label_block = train_test_split(
    all_text_data,
    final_class_labels,
    test_size=0.2,
    random_state=42
)


# converting raw text into numeric vectors using TF-IDF
# using unigrams and bigrams to capture small phrase patterns
tfidf_engine = TfidfVectorizer(
    stop_words='english',
    ngram_range=(1, 2)
)

train_feature_matrix = tfidf_engine.fit_transform(train_text_block)
test_feature_matrix = tfidf_engine.transform(test_text_block)


# ------------------ Model 1: Naive Bayes ------------------
nb_classifier_object = MultinomialNB()
nb_classifier_object.fit(train_feature_matrix, train_label_block)
nb_predictions = nb_classifier_object.predict(test_feature_matrix)


# ------------------ Model 2: Logistic Regression ------------------
lr_classifier_object = LogisticRegression(max_iter=1000)
lr_classifier_object.fit(train_feature_matrix, train_label_block)
lr_predictions = lr_classifier_object.predict(test_feature_matrix)


# ------------------ Model 3: Support Vector Machine ------------------
svm_classifier_object = LinearSVC()
svm_classifier_object.fit(train_feature_matrix, train_label_block)
svm_predictions = svm_classifier_object.predict(test_feature_matrix)


# ------------------ Evaluation Section ------------------

def show_model_performance(actual_labels, predicted_labels, title_name):

    print("\n====================================================")
    print("Model Name:", title_name)
    print("====================================================")

    # calculating overall accuracy separately
    overall_accuracy_value = accuracy_score(actual_labels, predicted_labels)
    print("Accuracy:", round(overall_accuracy_value, 4))

    # getting classification details as dictionary
    report_content = classification_report(
        actual_labels,
        predicted_labels,
        output_dict=True
    )

    # removing the automatic accuracy row because we print it above
    if "accuracy" in report_content:
        del report_content["accuracy"]

    report_table = pd.DataFrame(report_content).transpose()

    print("\nDetailed Classification Report:\n")
    print(report_table.round(4))
    print("\n")


# ---- Running evaluation for each model ----
show_model_performance(test_label_block, nb_predictions, "Naive Bayes")
show_model_performance(test_label_block, lr_predictions, "Logistic Regression")
show_model_performance(test_label_block, svm_predictions, "Support Vector Machine")



# function to display confusion matrix for a given model
def display_matrix(real_tags, predicted_tags, classifier_title):

    # computing confusion matrix using sklearn
    cm_values = confusion_matrix(real_tags, predicted_tags, labels=["SPORT", "POLITICS"])

    # setting figure size manually
    plt.figure(figsize=(5, 4))

    # drawing heatmap for better visualization
    sns.heatmap(
        cm_values,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=["SPORT", "POLITICS"],
        yticklabels=["SPORT", "POLITICS"]
    )

    print("\n=============================================================================\n")

    # adding titles and axis labels for clarity
    plt.title("Confusion Matrix - " + classifier_title)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")

    # adjusting layout to avoid label cutoff
    plt.tight_layout()
    plt.show()

    print("\n=============================================================================\n")


# calling confusion matrix for each trained model
display_matrix(test_label_block, nb_predictions, "Naive Bayes")
display_matrix(test_label_block, lr_predictions, "Logistic Regression")
display_matrix(test_label_block, svm_predictions, "Support Vector Machine")


# function to compare accuracy of all models in one bar chart
def show_accuracy_graph():

    # model names listed manually
    algo_names = ["Naive Bayes", "Logistic Regression", "SVM"]

    # calculating accuracy values individually
    algo_scores = [
        accuracy_score(test_label_block, nb_predictions),
        accuracy_score(test_label_block, lr_predictions),
        accuracy_score(test_label_block, svm_predictions)
    ]

    print("\n=============================================================================\n")

    # creating bar plot for comparison
    plt.figure(figsize=(6, 4))
    plt.bar(algo_names, algo_scores)

    # setting y-axis limit so differences are clearly visible
    plt.ylim(0.9, 1.0)

    plt.title("Model Accuracy Comparison")
    plt.ylabel("Accuracy")

    # rotating x-axis labels slightly for readability
    plt.xticks(rotation=20)

    plt.tight_layout()
    plt.show()

    print("\n=============================================================================\n")


# calling accuracy comparison plot
show_accuracy_graph()

